{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Example\n",
    "This notebook demonstrates how to use the pruning methods from this tool to compress a model. \n",
    "\n",
    "The example uses the MNIST dataset and a simple CNN model. The model is trained and then pruned using the methods in this tool. The pruned model is then evaluated on the test set to see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import importlib\n",
    "import inspect\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Add thesis package to path\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.general as general\n",
    "import src.compression.pruning as pruning\n",
    "import src.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    }
   ],
   "source": [
    "model_state = \"../models/mnist.pt\"\n",
    "model_class = \"models.mnist\"\n",
    "\n",
    "# Import the module classes\n",
    "module = importlib.import_module(model_class)\n",
    "classes = general.get_module_classes(module)\n",
    "for cls in classes:\n",
    "    globals()[cls.__name__] = cls\n",
    "\n",
    "# Get device\n",
    "device = general.get_device()\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_state, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == \"cuda\" else {}\n",
    "mnist_transform = transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=mnist_transform,),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=mnist_transform,),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train and test methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:20<00:00, 46.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== TRAIN PERFORMANCE ==============================\n",
      "Average loss = 0.3792\n",
      "Elapsed time = 20381.11 milliseconds (21.73 per batch, 0.68 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:01<00:00, 148.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== TEST PERFORMANCE ===============================\n",
      "Average loss = 0.1961\n",
      "Accuracy = 0.9396\n",
      "Elapsed time = 1059.20 milliseconds (6.75 per batch, 0.42 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:20<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== TRAIN PERFORMANCE ==============================\n",
      "Average loss = 0.0631\n",
      "Elapsed time = 20191.55 milliseconds (21.53 per batch, 0.67 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:01<00:00, 146.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== TEST PERFORMANCE ===============================\n",
      "Average loss = 0.0986\n",
      "Accuracy = 0.9702\n",
      "Elapsed time = 1073.09 milliseconds (6.83 per batch, 0.43 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:20<00:00, 46.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== TRAIN PERFORMANCE ==============================\n",
      "Average loss = 0.0688\n",
      "Elapsed time = 20213.10 milliseconds (21.55 per batch, 0.67 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:01<00:00, 147.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== TEST PERFORMANCE ===============================\n",
      "Average loss = 0.0678\n",
      "Accuracy = 0.9792\n",
      "Elapsed time = 1066.15 milliseconds (6.79 per batch, 0.42 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = F.nll_loss\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    general.train(model, device, train_loader, criterion, optimizer, metric=metrics.accuracy)\n",
    "    general.test(model, device, test_loader, criterion, metric=metrics.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "save_model_path = \"../models/mnist.pt\"\n",
    "\n",
    "if save_model:\n",
    "    torch.save(model, save_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "Pruning is a process of reducing the size of a machine learning model by removing unimportant weights and neurons. Pruning can be used to reduce the number of parameters in a model, thereby reducing the memory footprint and the computational complexity of the model. \n",
    "\n",
    "Pruning is typically done in two ways: structured pruning, which involves selectively removing a larger part of the network such as a layer or a channel, and unstructured pruning, which involves removing individual weights or neurons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured Pruning\n",
    "Here we prune individual weights or neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() missing 2 required positional arguments: 'criterion' and 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zw/6683d62105s8pxz87x2_ny900000gn/T/ipykernel_36099/486598545.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test model performance before pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Before pruning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the parameters to prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: test() missing 2 required positional arguments: 'criterion' and 'epoch'"
     ]
    }
   ],
   "source": [
    "# Test model performance before pruning\n",
    "print(\"Before pruning\")\n",
    "test(model, device, test_loader)\n",
    "\n",
    "# Define the parameters to prune\n",
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.conv2, 'weight'),\n",
    "    (model.fc1, 'weight'),\n",
    "    (model.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.95,\n",
    ")\n",
    "\n",
    "# Test model performance after pruning\n",
    "print(\"After pruning\")\n",
    "test(model, device, test_loader)\n",
    "\n",
    "# Print number of parameters in model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "\n",
    "\n",
    "# Print number of parameters pruned\n",
    "total_pruned_params = sum(p.numel() for p in model.parameters() if hasattr(p, 'mask'))\n",
    "print(f'{total_pruned_params:,} parameters pruned ({100 * total_pruned_params / total_params:.2f}% pruned)')\n",
    "\n",
    "# Print number of parameters remaining\n",
    "total_unpruned_params = sum(p.numel() for p in model.parameters() if not hasattr(p, 'mask'))\n",
    "print(f'{total_unpruned_params:,} parameters unpruned ({100 * total_unpruned_params / total_params:.2f}% unpruned)')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff67d639abb31abb6a46275810293efc60456a6edbd614d8502142bf104bd3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
