{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "This notebook shows how the tool can be used to perform knowledge distillation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "* Import dependencies\n",
    "* Import data loaders\n",
    "* Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import inspect\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add thesis package to path\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.general as general\n",
    "import src.compression.distillation as distill\n",
    "import src.metrics as metrics\n",
    "import src.evaluation as eval\n",
    "import src.plot as plot\n",
    "from models.mnist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "use_cuda = False\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "mnist_transform = transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=mnist_transform,),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=mnist_transform,),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    }
   ],
   "source": [
    "model_state = \"../models/mnist.pt\"\n",
    "\n",
    "device = general.get_device()\n",
    "teacher_model = torch.load(model_state, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "The original model acts as the teacher model. \n",
    "\n",
    "For the student model the user can either give a model architecture of their own, presented in a `.py` file, or use the the tool to intelligently design a student model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the student model\n",
    "student_model = MnistSmallLinear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = next(iter(test_loader))\n",
    "example_input = input_batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:01<00:00, 30.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 0.0010\n",
      "Metric = 7.5550\n",
      "Elapsed time = 1975.40 milliseconds (32.92 per batch, 0.03 per data point)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test performance of student model before training\n",
    "loss, score, duration, batch_duration, data_duration = general.test(student_model, device,  test_loader, criterion=F.nll_loss, metric = lambda x,y: metrics.accuracy_topk(x,y,topk=(1,))[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================== METRICS ===================================\n",
      "Loss: 0.001031\n",
      "Score: 7.555000\n",
      "Time per batch: 32.9233 ms (64 per batch)\n",
      "Time per data point: 0.0329 ms\n",
      "Model Size: 0.15 MB\n",
      "Number of parameters: 39760\n",
      "Number of FLOPS: 39.75K\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "before_training_student_evaluation_metrics = {\n",
    "    \"loss\": loss,\n",
    "    \"score\": score,\n",
    "    \"duration\": duration,\n",
    "    \"batch_duration\": batch_duration,\n",
    "    \"data_duration\": data_duration,\n",
    "    \"model\": student_model,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"example_input\": example_input,\n",
    "}\n",
    "\n",
    "plot.print_metrics(**before_training_student_evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distillation Training: 100%|██████████| 938/938 [00:08<00:00, 115.56it/s]\n",
      "Distillation Validation: 100%|██████████| 60/60 [00:01<00:00, 31.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Distillation loss: 10.551363945007324\n",
      "Test loss: 0.34507241000731786, Test accuracy: 0.9132666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distillation Training: 100%|██████████| 938/938 [00:08<00:00, 113.07it/s]\n",
      "Distillation Validation: 100%|██████████| 60/60 [00:01<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Distillation loss: 8.376518249511719\n",
      "Test loss: 0.24120648329456648, Test accuracy: 0.93515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distillation Training: 100%|██████████| 938/938 [00:08<00:00, 116.86it/s]\n",
      "Distillation Validation: 100%|██████████| 60/60 [00:01<00:00, 31.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Distillation loss: 8.546671867370605\n",
      "Test loss: 0.22435658077398937, Test accuracy: 0.9376833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distillation Training: 100%|██████████| 938/938 [00:08<00:00, 112.65it/s]\n",
      "Distillation Validation: 100%|██████████| 60/60 [00:02<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Distillation loss: 11.111848831176758\n",
      "Test loss: 0.2286367081105709, Test accuracy: 0.9371333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distillation Training: 100%|██████████| 938/938 [00:08<00:00, 107.03it/s]\n",
      "Distillation Validation: 100%|██████████| 60/60 [00:01<00:00, 31.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Distillation loss: 9.468725204467773\n",
      "Test loss: 0.20371440450350445, Test accuracy: 0.9424666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=lr) # Important: use the student model parameters\n",
    "distil_criterion = F.mse_loss\n",
    "eval_criterion = F.cross_entropy\n",
    "\n",
    "\n",
    "distill.distillation_train_loop(teacher_model, student_model, train_loader, test_loader, distil_criterion, eval_criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:01<00:00, 31.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 1.3820\n",
      "Accuracy = 0.9425\n",
      "Elapsed time = 1924.83 milliseconds (32.08 per batch, 0.03 per data point)\n",
      "================================================================================\n",
      "============================= METRICS BEFORE & AFTER ===========================\n",
      "Loss: 0.001031 -> 1.381984\n",
      "Score: 7.555000 -> 0.942467 \n",
      "Time per batch: 32.9233 ms -> 32.0805 ms (64 per batch)\n",
      "Time per data point: 0.0329 ms -> 0.0321 ms\n",
      "Model Size: 0.15 MB -> 0.15 MB\n",
      "Number of parameters: 39760 -> 39760\n",
      "Number of FLOPS: 39.75K -> 39.75K\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after training\n",
    "print(\"Student model performance:\")\n",
    "loss, score, duration, batch_duration, data_duration = general.test(student_model, device,  test_loader, criterion=F.nll_loss, metric = metrics.accuracy)\n",
    "after_training_student_evaluation_metrics = {\n",
    "    \"loss\": loss,\n",
    "    \"score\": score,\n",
    "    \"duration\": duration,\n",
    "    \"batch_duration\": batch_duration,\n",
    "    \"data_duration\": data_duration,\n",
    "    \"model\": student_model,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"example_input\": example_input,\n",
    "}\n",
    "plot.print_before_after_metrics(before_training_student_evaluation_metrics, after_training_student_evaluation_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Analyze the metrics of the new student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:04<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 0.0174\n",
      "Accuracy = 0.9946\n",
      "Elapsed time = 4808.84 milliseconds (80.15 per batch, 0.08 per data point)\n",
      "================================================================================\n",
      "==================================== METRICS ===================================\n",
      "Loss: 0.017429\n",
      "Score: 0.994583\n",
      "Time per batch: 80.1473 ms (64 per batch)\n",
      "Time per data point: 0.0801 ms\n",
      "Model Size: 1.65 MB\n",
      "Number of parameters: 431080\n",
      "================================================================================\n",
      "Student model performance:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:01<00:00, 31.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 1.3820\n",
      "Accuracy = 0.9425\n",
      "Elapsed time = 1911.41 milliseconds (31.86 per batch, 0.03 per data point)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "============================= METRICS BEFORE & AFTER ===========================\n",
      "Loss: 0.017429 -> 1.381984\n",
      "Score: 0.994583 -> 0.942467 \n",
      "Time per batch: 80.1473 ms -> 31.8569 ms (64 per batch)\n",
      "Time per data point: 0.0801 ms -> 0.0319 ms\n",
      "Model Size: 1.65 MB -> 0.15 MB\n",
      "Number of parameters: 431080 -> 39760\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model performance after distillation\n",
    "print(\"Teacher model performance:\")\n",
    "loss, score, duration, batch_duration, data_duration = general.test(teacher_model, device, test_loader, criterion=F.nll_loss, metric = metrics.accuracy)\n",
    "teacher_evaluation_metrics = {\n",
    "    \"model\": teacher_model,\n",
    "    \"loss\": loss,\n",
    "    \"score\": score,\n",
    "    \"duration\": duration,\n",
    "    \"batch_duration\": batch_duration,\n",
    "    \"data_duration\": data_duration,\n",
    "    \"batch_size\": batch_size,\n",
    "    # \"example_input\": example_input,\n",
    "}\n",
    "plot.print_metrics(**teacher_evaluation_metrics)\n",
    "print(\"Student model performance:\")\n",
    "loss, score, duration, batch_duration, data_duration = general.test(student_model, device, test_loader, criterion=F.nll_loss, metric = metrics.accuracy)\n",
    "student_evaluation_metrics = {\n",
    "    \"model\": student_model,\n",
    "    \"loss\": loss,\n",
    "    \"score\": score,\n",
    "    \"duration\": duration,\n",
    "    \"batch_duration\": batch_duration,\n",
    "    \"data_duration\": data_duration,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"example_input\": example_input,\n",
    "}\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# # Compare the number of parameters of the teacher and student model\n",
    "# teacher_params = eval.get_model_parameters(teacher_model)\n",
    "# student_params = eval.get_model_parameters(student_model)\n",
    "# print('Number of parameters: {} (Teacher) -> {} (Student)'.format(teacher_params, student_params))\n",
    "\n",
    "# # Compare the model size of the teacher and student model\n",
    "# teacher_size = eval.get_model_size(teacher_model)\n",
    "# student_size = eval.get_model_size(student_model)\n",
    "# print('Model Size: {} MB (Teacher) -> {} MB (Student)'.format(teacher_size, student_size))\n",
    "\n",
    "\n",
    "plot.print_before_after_metrics(teacher_evaluation_metrics, student_evaluation_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff67d639abb31abb6a46275810293efc60456a6edbd614d8502142bf104bd3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
