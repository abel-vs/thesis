{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "This notebook acts as an example of how to use the quantization techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "* Import the necessary packages.\n",
    "* Load a model.\n",
    "* Load a dataset.\n",
    "* Analyze performance of the model prior to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import inspect\n",
    "import sys\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Add thesis package to path\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import src.general as general\n",
    "import src.metrics as metrics\n",
    "import src.evaluation as eval\n",
    "import src.compression.quantization as quant\n",
    "import src.plot as plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    }
   ],
   "source": [
    "model_state = \"../models/mnist.pt\"\n",
    "model_class = \"models.mnist\"\n",
    "\n",
    "# Import the module classes\n",
    "module = importlib.import_module(model_class)\n",
    "classes = general.get_module_classes(module)\n",
    "for cls in classes:\n",
    "    globals()[cls.__name__] = cls\n",
    "\n",
    "# Get device\n",
    "device = general.get_device()\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_state, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "use_cuda = False\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "mnist_transform = transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=mnist_transform,),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=mnist_transform,),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Quantization Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:04<00:00, 14.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 0.0767\n",
      "Metric = 0.9771\n",
      "Elapsed time = 4218.61 milliseconds (70.31 per batch, 0.07 per data point)\n",
      "================================================================================\n",
      "==================================== RESULTS ===================================\n",
      "Loss: 0.076667\n",
      "Score: 0.977067\n",
      "Time per data point: 0.0703 ms\n",
      "Model Size: 1.65 MB\n",
      "Number of parameters: 431080\n",
      "Number of FLOPs: 2307720\n",
      "Number of MACs: 2307720.51\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = F.nll_loss\n",
    "metric = metrics.accuracy\n",
    "\n",
    "before_results = eval.get_results(model, test_loader, criterion, metric)\n",
    "plot.print_results(**before_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization\n",
    "Here the model’s weights are pre-quantized; the activations are quantized on-the-fly (“dynamic”) during inference. \n",
    "\n",
    "Currently only Linear and Recurrent (LSTM, GRU, RNN) layers are supported for dynamic quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_quantized_model = quant.dynamic_quantization(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 60/60 [00:04<00:00, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 0.0766\n",
      "Metric = 0.9773\n",
      "Elapsed time = 4385.61 milliseconds (73.09 per batch, 0.07 per data point)\n",
      "================================================================================\n",
      "============================= RESULTS BEFORE & AFTER ===========================\n",
      "Loss: 0.076667 -> 0.076579\n",
      "Score: 0.977067 -> 0.977283 \n",
      "Time per data point: 0.0703 ms -> 0.0731 ms\n",
      "Model Size: 1.65 MB -> 0.49 MB\n",
      "Number of parameters: 431080 -> 25570\n",
      "Number of FLOPs: 2307720 -> 1902720\n",
      "Number of MACs: 2307720.51 -> 1902720.0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_quantized_results = eval.get_results(dynamic_quantized_model, test_loader, criterion, metric)\n",
    "plot.print_before_after_results(before_results, dynamic_quantized_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantization\n",
    "Post Training Static Quantization (PTQ) also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (“static”) using validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 938/938 [00:06<00:00, 135.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ TEST PERFORMANCE ==============================\n",
      "Average loss = 0.0767\n",
      "Elapsed time = 6917.16 milliseconds (7.37 per batch, 0.23 per data point)\n",
      "================================================================================\n",
      "Static Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/abel/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/ao/quantization/utils.py:280: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "static_quantized_model = quant.static_quantization(model, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Metal, Negative, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID].\n\nMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/mps/MPSFallback.mm:25 [backend fallback]\nQuantizedCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/native/quantized/cpu/qconv.cpp:1423 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback]\nNamed: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/torch/csrc/autograd/TraceTypeManual.cpp:295 [backend fallback]\nAutocastCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/autocast_mode.cpp:481 [backend fallback]\nAutocast: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:89 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zw/6683d62105s8pxz87x2_ny900000gn/T/ipykernel_88169/1443408510.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstatic_quantized_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_quantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_before_after_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_quantized_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/thesis/examples/../src/evaluation.py\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(model, test_loader, criterion, metric)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_and_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     results = {\n",
      "\u001b[0;32m~/Development/thesis/examples/../src/evaluation.py\u001b[0m in \u001b[0;36mtest_and_get_metrics\u001b[0;34m(model, test_loader, criterion, metric)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     loss, score, duration, batch_duration, data_duration = general.test(\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     )\n",
      "\u001b[0;32m~/Development/thesis/examples/../src/general.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, criterion, metric)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/thesis/examples/../mnist.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [1, 28, 28] -> [20, 24, 24]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [20, 24, 24] -> [20, 12, 12]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [20, 12, 12] -> [50, 8, 8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    454\u001b[0m             input = F.pad(input, _reversed_padding_repeated_twice,\n\u001b[1;32m    455\u001b[0m                           mode=self.padding_mode)\n\u001b[0;32m--> 456\u001b[0;31m         return ops.quantized.conv2d(\n\u001b[0m\u001b[1;32m    457\u001b[0m             input, self._packed_params, self.scale, self.zero_point)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Metal, Negative, UNKNOWN_TENSOR_TYPE_ID, QuantizedXPU, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseCPU, SparseCUDA, SparseHIP, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, SparseVE, UNKNOWN_TENSOR_TYPE_ID, NestedTensorCUDA, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID, UNKNOWN_TENSOR_TYPE_ID].\n\nMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/mps/MPSFallback.mm:25 [backend fallback]\nQuantizedCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/native/quantized/cpu/qconv.cpp:1423 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/PythonFallbackKernel.cpp:133 [backend fallback]\nNamed: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]\nAutogradCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]\nAutogradCUDA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]\nAutogradXLA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]\nAutogradMPS: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]\nAutogradXPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]\nAutogradHPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]\nAutogradLazy: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]\nTracer: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/torch/csrc/autograd/TraceTypeManual.cpp:295 [backend fallback]\nAutocastCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/autocast_mode.cpp:481 [backend fallback]\nAutocast: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFunctionalize: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:89 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484780698/work/aten/src/ATen/core/PythonFallbackKernel.cpp:137 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "static_quantized_results = eval.get_results(static_quantized_model, test_loader, criterion, metric)\n",
    "plot.print_before_after_results(before_results, static_quantized_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff67d639abb31abb6a46275810293efc60456a6edbd614d8502142bf104bd3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
